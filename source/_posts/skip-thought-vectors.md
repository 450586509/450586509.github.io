---
title: skip-thought vectors
date: 2019-12-16 21:02:45
categories: nlp
tags: [nlp, word-embedding]
---

### 问题描述
本文实现了一种基于无监督的句子向量表示方法。常见的句子表示方法有基于词向量和语言模型的方法，比如加权word embedding、bert和xlnet。语言模型有AR（auto recursive）和AE(auto endoder)两种。本文其实也用到了AE的思想，与bert不同的是，本文利用中间句子的向量表示生成上下两句文本。本文名叫skip-thought vector，可能也借用了skip-gram的思想，skip-gram利用当前词预测上下文中的词，本文用当前句子预测上下两句。这种引申的思想还是挺实用的，在数学研究中也会经常见到此类做法，比如把概念从低维度引申到高维度。实验部分，作者在8个任务上测试了模型的效果，都有不错的效果和鲁棒性。
<!-- more -->
### 模型介绍
本文的模型比较简单易懂。形象描述如下图：
![](1.jpg)
模型一共由三个GRU网络组成。记三个连续的句子为$s_{i-1},s_i,s_{i+1}$，那么${s_i}$可以对应为上图的"I Could see the cat on the steps", $s_{i-1}$为上图的“<eos> I got back home”, $s_{i+1}$为上图的“<eos> this was strange”。将$s_i$输入到第一个GRU，将最后的隐含状态作为第二个和第三个GRU的输入，第2个GRU重构$s_{i-1}$，第3个GRU重构$s_{i+1}$。最终的损失函数为：
![](2.jpg)
$h_i$代表第一个GRU的最后的隐含向量，也是中间句子的向量表示。如果了解基于lstm的语言模型，这个损失函数应该很好理解。
### Vocabulary expansion
本文提出的解决OOV(out of vocabulary)的方案也挺有意思。与常见的处理OOV方法不同，本文学习了一种映射模型，可以将未见过的词映射为某个可能的向量表示。在测试或者线上部署模型时，很有可能会出现OOV的情况。假设本文模型学习到的词向量为$V_{rnn}$，如果你有另一个词汇量很大的词向量$V_{w2n}$，比如google开源的大规模词向量。基于这两个词向量可以训练一个映射模型，即：
$$f:V_{w2v}\rightarrow V_{rnn}$$
$$v^{'}=Wv$$
其中$v^{'}$是$V_{rnn}$中的词，$v$是$V_{w2v}$中的词，$W$是要学习的参数。通过L2线性回归损失函数，求得映射矩阵$W$。

### 创新点
- 本文提出了一种有效的句子表示方法，并且在多个任务上取得不错的效果。作者最后提到，如果使用更加复杂的Encoder和Decoder，可能取得更好的效果。
- 本文提出了一种新的解决OOV的方法。方法简洁易懂。
