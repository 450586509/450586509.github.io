<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[skip-thought vectors]]></title>
      <url>%2F2019%2F12%2F16%2Fskip-thought-vectors%2F</url>
      <content type="text"><![CDATA[问题描述本文实现了一种基于无监督的句子向量表示方法。常见的句子表示方法有基于词向量和语言模型的方法，比如加权word embedding、bert和xlnet。语言模型有AR（auto recursive）和AE(auto endoder)两种。本文其实也用到了AE的思想，与bert不同的是，本文利用中间句子的向量表示生成上下两句文本。本文名叫skip-thought vector，可能也借用了skip-gram的思想，skip-gram利用当前词预测上下文中的词，本文用当前句子预测上下两句。这种引申的思想还是挺实用的，在数学研究中也会经常见到此类做法，比如把概念从低维度引申到高维度。实验部分，作者在8个任务上测试了模型的效果，都有不错的效果和鲁棒性。 模型介绍本文的模型比较简单易懂。形象描述如下图：模型一共由三个GRU网络组成。记三个连续的句子为$s_{i-1},s_i,s_{i+1}$，那么${s_i}$可以对应为上图的”I Could see the cat on the steps”, $s_{i-1}$为上图的“ I got back home”, $s_{i+1}$为上图的“ this was strange”。将$s_i$输入到第一个GRU，将最后的隐含状态作为第二个和第三个GRU的输入，第2个GRU重构$s_{i-1}$，第3个GRU重构$s_{i+1}$。最终的损失函数为：$h_i$代表第一个GRU的最后的隐含向量，也是中间句子的向量表示。如果了解基于lstm的语言模型，这个损失函数应该很好理解。 Vocabulary expansion本文提出的解决OOV(out of vocabulary)的方案也挺有意思。与常见的处理OOV方法不同，本文学习了一种映射模型，可以将未见过的词映射为某个可能的向量表示。在测试或者线上部署模型时，很有可能会出现OOV的情况。假设本文模型学习到的词向量为$V_{rnn}$，如果你有另一个词汇量很大的词向量$V_{w2n}$，比如google开源的大规模词向量。基于这两个词向量可以训练一个映射模型，即：$$f:V_{w2v}\rightarrow V_{rnn}$$$$v^{‘}=Wv$$其中$v^{‘}$是$V_{rnn}$中的词，$v$是$V_{w2v}$中的词，$W$是要学习的参数。通过L2线性回归损失函数，求得映射矩阵$W$。 创新点 本文提出了一种有效的句子表示方法，并且在多个任务上取得不错的效果。作者最后提到，如果使用更加复杂的Encoder和Decoder，可能取得更好的效果。 本文提出了一种新的解决OOV的方法。方法简洁易懂。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[learning semantic hierarchies via word embedding]]></title>
      <url>%2F2019%2F12%2F10%2Flearning-semantic-hierarchies-via-word-embedding%2F</url>
      <content type="text"><![CDATA[问题描述本文基于词向量技术，提出一种新的概念语义层（Semantic hierarchy）构建方法。概念语义层是一种树状结构，节点表示概念，边表示相连的两个概念存在上下位关系（is-a）的关系，下图是一个例子。概念间的上下位关系在知识图谱Schema构建时会用到。本文提出的方法F-score有73.74%，取得最佳的效果，将本方法和之前人工设计的一种方法结合，可以进一步将F-score提升到80.29%。 Motivation本文的灵感来源非常有意思，我们以前都见过这个例子：$v(king)-v(queue) \approx v(man) - v(woman)$，作者发现上下位词对也存在这种关系：假设$(x,y)$存在上下位关系，且x是y的下位词，那么y是x的上位词，例如：x=对虾，y=虾。如果把所有上下位词对中的上位词减去下位词，得到offset向量，再将offset向量做聚类，会得到如下效果：由上图可见offset向量存在明显的聚类关系。作者最初希望找到一个矩阵$W$，使得$Wx=y$，既把x的词向量经过变换，变成y。这种技巧在很多地方都有用到，例如trans系列。估计发现只用一个W估计效果不佳，所以为每个聚类训练一个映射矩阵$W$，每一个聚类中上下位词对（x,y）用各自的映射矩阵，同一个聚类中的词对用同一个映射矩阵，该矩阵可以把下位词向量映射为上位词词向量。 loss函数本文的词向量可以使用已经训练好的词向量，需要训练的对象是映射矩阵$W$，损失为：$N_k$代表第k个聚类中上下位词对的数目，$C_k$代表第k个聚类中的上下位词对的集合，$\Phi$代表映射矩阵。 模型的作用用于判断两个词是否存在上下位关系。如果存在某个映射矩阵$\Phi_{k}$使得(x,y)满足：$$|\Phi_{k}x - y|&lt;\sigma$$$\sigma$是一个阈值。则表示(x,y)满足上下位关系。给定一个词典，如果知道任意两个词的是否存在上下位关系，那么可以构造一棵概念语义层次树。 创新点：本文是刘知远教授推荐的一篇文章，他表示自己非常喜欢文中模型的创意，他评价说该文，思路简单，方法高效，论文图示赏心悦目。这应该是首个充分利用word2vec的king-queen=man-woman特性的研究工作。的确，本文从king-queen=man-woman这个现象触发，发现上下位词对也存在类似的性质，并通过可视化的方法，发现offset向量存在明显的聚类关系。所以，有时多角度看问题，能有意想不到的收获，比如文中通过可视化角度展示offset向量发现聚类关系。再利用一个映射矩阵，把下位词向量映射到上位词，方法简单直观。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Delete, Retrieve, Generate: A simple Approach to Sentiment and Style Transfer -- percy liang]]></title>
      <url>%2F2019%2F11%2F30%2FDelete-Retrieve-Generate-A-simple-Approach-to-Sentiment-and-Style-Transfer%2F</url>
      <content type="text"><![CDATA[问题描述本文旨在解决句子风格变换的问题。句子风格种类已知，给定一个句子和目标风格，生成一个符合目标风格的新句子。新句子和原句子描述的类容一样，差别在于描述风格的用词。下面是一个将评论语句由积极转为消极的例子，新句子和元句子的风格发生了变化，但是所评价的对象和原句一致，两句都是在评论food、staff和worker。 原句子： great food but horrible staff and very rude workers！新句子：great food , awesome staff, very personable and efficient atmosphere。 解决方案动机作者发现，句子的风格通常由表示风格的片段体现（distinctive phrase），例如“too small， rude，bad”。所以，本文期望从原句中找到描述句子风格的片段，并替换成描述目标风格的片段，便可以在保证描述对象不变的情况下，实现句子的风格转换。本文提出了四种无监督算法，这四种算法的思路大致一样，其中DelteAndRetrieve取得最佳的效果，其它三种算法用作对比和Baseline。DeleteAndRetrieve主要包括以下几步： 符号含义：输入句子$s$，当前风格$v^{s}$，目标风格$v^{t}$，$s$去掉风格描述片段后剩下的内容$c(s)$，风格描述片段$p$。 首先，从$s$中抽取出描述的片段$p$，并可以得到$c(s)$。 从风格为$v^{t}$的句子集合中，找一个与$s$相似的句子$s_r$。 从句子$s_r$中抽取出表示风格的片段$p_r$。 利用已经训练好的生成模型，以$c(s)$和$p_r$为输入，便可生成风格为$v^{t}$新句子。 四种方案的流程图如下： 简单介绍其它三种方案：RetrieveOnly: 使用相似性检索算法，从目标属性的集合中，检索出与原句子最相似的句子，作为目标句子。这种方案可以保证语法准确和语义完整，并且得到目标风格的句子，但是风格片段的描述对象可能发生变化。 TemplateBased: 利用被检索出句子的风格描述片段，代替原句子中的风格描述片段，得到目标句子。这种方案得到的结果，能够保证得到目标风格的句子，并且风格片段的描述对象保存一致，但是由于采用直接替换的方式，可能导致语法错误的问题。 DeleteOnly: 不使用检索操作，直接将$c(s)$和目标风格$v^{t}$输入RNN生成模型中，得到新的句子。 方案细节Delete为了在句子$s$中找到与风格相关的片段，论文中定义了$S(u,v)$值，该值用来表示片句子段$u$与风格$v$的相关性，$S(u,v)$的值与$u$和$v$的相关性成正比。计算公式为$$S(u,v)=\frac{count(u,D_v) + \lambda}{\sum_{v^{‘}\in V,v^{‘}\neq v}count(u,D_{v^{‘}})+\lambda}$$ $D_v$是具有风格$v$的句子集，计算$S(u,v)$是一个统计的过程，先统计片段$u$在$D_v$中的出现次数，再统计片段$u$在其它属性的句子集合中出现次数，$\lambda$是一个超参数。 Retrieve利用$c(s)$,即去掉风格片段后的句子，在目标风格的句子集合中搜索与$c(s)$最相似的句子。在计算相似性时，都利用去掉风格的句子计算。论文中使用的相似性算法有基于tf-idf的词重叠算法（求杰卡德系数）和基于tf-idf加权的词向量句子表示的欧式距离。 Generate以下分别描述四个模型的生成过程： RetrieveOnly:直接使用检索出的最相似句子当作生成的句子。 TemplateBased:利用被检索出句子的风格描述片段，代替原句子中的风格描述片段，得到目标句子。 DeleteOnly:利用RNN把$c(s)$转换成向量，取最后的隐含状态$s_t$；再把目标风格$a_{target}$embedding成向量，目标风格相当于一个word；再把两个向量拼接，输入到另一个RNN中，生成目标句子。该方案说明可以通过属性向量，控制生成句子的风格。 DeleteAndRetrieve: 与DeleteOnly的区别在于，这种方法不使用目标风格，而使用目标风格的描述片段。首先，利用RNN将$c(s)$转换成向量；再用另一个RNN把目标风格片段，转换成向量；接着将两个向量拼接，输入到另一个RNN,得到新的句子。从DeleteOnly和DeleteAndRetrieve两种方法来看，可以利用向量拼接的方法，融合不同的输入信息. 实现细节生成模型的训练在DeleteOnly和DeleteAndRetrieve两个模型中，都用到Encoder-Decoder架构的模型，那么两个方案的模型是如何训练的？ DeleteOnly: 为了训练Encoder-Decoder模型，需要平行语聊对。本方案使用了类似AutoEncoder的方法训练模型。模型的输入是句子$s$的$c(s)$和风格$a_{s}$，输出原句$s$，即重构原句$s$。 DeleteAndRetrieve：与DeleteOnly类似，训练DeleteAndRetrieve模型时的输入是$c(s)$和句子$s$的风格描述片段（记为$p_s$），输出原句$s$，也是重构$s$。所以都是用到AutoEncoder的思想。但是本模型会对$s$的风格描述片段以0.1的概率加入噪声，如果存在相同风格的风格描述片段$q_s$,且$p_s$与$q_s$的编辑距离为1,则替换，否则继续使用$p_s$。目的是使模型具有更好的鲁棒性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pun generation with surprise -- percy liang]]></title>
      <url>%2F2019%2F11%2F24%2Fpun-generation-with-surprise%2F</url>
      <content type="text"><![CDATA[问题描述介绍一种无监督方法生成音同或音近的双关语句子（pun sentence）的方案。相比于有监督方法，该可以可复训练语料不足的问题，并且更有创新型。经人工评估后表示，该方案成功生成双关语句子的概率有33%，超过基于神经网络的基准方案的3倍概率。 解决方案什么造就一个好的双关语句子？作者发现，当我们在阅读一个句子的某个词的时候，基于上文大家会这个词的可能性有个估计。对于一个双关词，从较短的上下文来看，大家可能较难想到（发生概率小）。但是，从更长的上下文来看，大家又觉得应该是这样（发生概率大）。基于此发现，作者提出了一个local-global surprisal 原则，双关词在local上下文更让人surprising(意外)，但是在global 上下文中却显得情理之中。 双关语例子文中举了一个例子：Yesterday I accidentally swallowed some food coloring. The doctor says I’m OK, but I feel like I’ve （） a little inside。大家认为括号中填died的可能性更大一些，此处对应的双关词是dyed。从近的上文来看，p(dyed| but I feel like I’ve a little )很小；但是从更长的山下问来看，p(dyed|Yesterday I … food coloring)的概率较大。 A Local-Global Surprisal Measure。本文提出了量化local-global surprisal原则的方法：$S(c)=-log(\frac{p(w^p|c)}{p(w^a|c)})$$S_{local}=S(x_{p-d:p-1},x_{p+1:p+d})$$S_{global}=S(x_{1:p-1},x_{p+1:n})$$S_{ratio} = \begin{cases} -1 &amp; S_{local}&lt;0 or S_{global}&lt;0\ S_{local}/S_{global} &amp; 其它 \end{cases}$符号含义: c: 上下文 $w^p$: 双关词 $w^a$: 替代此 p: $w^p$的位置下标 该指标可以用来评估一个句子是否为双关句，一个双关语句子的$S_{ratio}$越大，代表效果越好。 方案总体流程如下：方案的输入数据： 语料：要来生成双关句 ($w^a,w^p$)：替代词和双关词。 local surprisal利用$w^a$从语料中检索一批候选句子。保留只有一个$w^a$的句子，并$w^a$的位置排序，越靠后越好。再把$w^p$代替$w^a$，这样会得到局部surprisal的效果。 global surprisal第一步用pun word (双关词)替代了相关词（alternative word）,这一步目是找到一个topic word，替换掉句子开头的一个词，起到预测pun word的作用。topic world 利用语言模型基于pun word生成。 Type consistent constraint目的是确保被删除的词和topic word具有相同的类型。基于WordNet path 相似性，判断两个词是否为相同类型。 例如，person和passenger是相同的类型，person和ship不是。被替换的词是句子中的第一个名词或者代词。 Improve grammaticality直接用topic word替换被替换的词，容易引起语法错误。本文应用一个seq2seq模型解决该问题。seq2seq是一个denoising autoencoders，把topic word周围的词删去，然后重构该句子。 创新亮点 提出了一种无监督生成双关句子的方案，并取得不错的效果。 Local-Global Surprisal 原则的发现和量化方法。 提升句子语法性的方法。 实验结果本文设计了两个实验： 验证local-grobal surprisal principle 假设的有效性。 比较双关句子生成模型和其它模型的效果。 总结展望 本文基于local-global surprisal 原则实现了一个双关句子生成模型。 验证了基于non-humorous文本训练的语言模型，可以得到一个检测双关句子的模型。 尽管我们的方法比其它模型要好，但是离人类生成的双关句子有很大差距。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[知识图谱构建]]></title>
      <url>%2F2019%2F05%2F04%2F%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E6%9E%84%E5%BB%BA%2F</url>
      <content type="text"><![CDATA[1. 什么是知识图谱？&emsp;&emsp;知识图谱是结构化的语义知识库^1，用于描述物理世界中的概念和概念之间的关系。&emsp;&emsp;知识图谱逻辑上由点、边组成，点代表实体，边表示实体之间的关系。实体还有很有很多属性，通过这些属性可以表现实体的特点。 2. 知识图谱的应用&emsp;&emsp;1. 基于知识图谱的问答。&emsp;&emsp;2. 搜索引擎。快速给出相关的实体。 3. 技术架构&emsp;&emsp;知识图谱是一个不断迭代的过程，可以归纳三个阶段：&emsp;&emsp;1. 知识抽取。从原始数据中抽取出实体、关系和属性。根据业务需求，可能会做属性和属性值的归一化处理。&emsp;&emsp;2. 知识融合。知识融合是指把一份新抽取的数据加入到已有知识库中。融合主要包括实体对齐和实体链接。知识抽取的结果中，同一个实体可能有有多份数据，而且知识库中可能也有该实体，所以需要使用实体对齐技术进行融合，从而得到唯一一个实体，既可以去除重复的实体，也能够在融合的过程中，给实体补充属性；知识抽取得到的通常是独立实体，需要通过链接技术，建立实体与知识库中实体之间的关系。&emsp;&emsp;3. 知识评估。对上述两步的结构，进行评估。 4. 构建技术&emsp;&emsp;下面介绍知识抽取和知识融合中用到的技术。 知识抽取&emsp;&emsp;由于知识抽取的任务是，从原始数据中提取实体、关系和属性。所以主要用到以下技术：&emsp;&emsp;1. 实体识别。抽取什么类型的实体，通常与业务场景相关。&emsp;&emsp;2. 关系抽取。&emsp;&emsp;3. 属性抽取。 知识融合&emsp;&emsp;1. 实体链接。&emsp;&emsp;1. 实体对齐。 Schema&emsp;&emsp;Schema相当于领域内的数据模型，定义了概念和概念的属性。在定义某个领域的Schema的时候，首先确定有哪些概念，再考虑对业务有用的概念属性。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PCFG和CCG简介]]></title>
      <url>%2F2019%2F05%2F03%2FPCFG%E5%92%8CCCG%E7%AE%80%E4%BB%8B%2F</url>
      <content type="text"><![CDATA[1. 什么是形式语法？&emsp;&emsp;Chomsky把形式语法理解为数目有限的规则的集合，这些规则可以生成语言中合格的句子，并排除不合格的句子。Chomsky把形式语法G定义为4元组。$G=(V_n,V_t,S,P)$，含义如下：&emsp;&emsp;1. $V_n:$ 非终结符集合&emsp;&emsp;2. $V_t:$ 终结符集合&emsp;&emsp;3. $S:$ $V_n$的初始符号&emsp;&emsp;4. $P:$ 重写规则。通过这些规则生成句子。 2. 什么是Chomsky层级？&emsp;&emsp;Chomsky根据重写规则的形式，把形式语法分为4类。含义如下，其中符号的含义是，${\phi}$、$\psi$和$w$都是符号串，$A$和$Q$为非终结符，$a$为终结符。&emsp;&emsp;1. 0型语法。满足重写规则：${\phi}{\rightarrow }\psi$。&emsp;&emsp;2. 上下文有关语法。满足重写规则：${\phi}_1A{\phi}_2{\rightarrow }{\phi}_1w{\phi}_2$。&emsp;&emsp;3. 上下文无关语法。满足重写规则：${A}{\rightarrow }w$，这种规则没有上下文限制。&emsp;&emsp;4. 有限状态语法。满足重写规则：$A{\rightarrow }aQ$或者$A{\rightarrow }a$。 3. 什么是PCFG？&emsp;&emsp;PCFG全称叫概率上下文无关语法（Probabilistic Context-Free Grammar）。在上下文无关语法的基础上，为每一个重写规则增加了一个条件概率。 4. PCFG中重写规则的概率如何获得？&emsp;&emsp;通过语料库统计得到，或者通过训练语料进行学习。 5. 什么是CCG？&emsp;&emsp;CCG全称叫组合范畴语法（Combinatory Categorical Grammar），对范畴语法进行了扩展。扩展的实质在于组合，在范畴语法的基础上，增加了函子范畴的组合运算。&emsp;&emsp;范畴语法把英语中的词用S和n两个最基本的范畴表示，n代表名词的句法类型，S表示句子。S和n代表原子范畴，其它词可以通过3个规则，用n和S表示其在句子中的句法类型。句法类型大致相当于传统语法中的词类。&emsp;&emsp;CCG的基本思路：&emsp;&emsp;1. 为每一个词或者短语赋一个类别。&emsp;&emsp;2. 再对类别按照组合规则进行组合。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[xgboost]]></title>
      <url>%2F2019%2F05%2F02%2Fxgboost%2F</url>
      <content type="text"><![CDATA[xgboost如何划分连续变量？由于训练集中样本的数量是有限的，所以也可以当作离散。在选择分裂点事，先把连续特征的取值从小到大排列，得到$[v_1,v_2,v_3,…,v_k]$,则分裂点可以选取为$[\frac{v_1+v_2}{2},\frac{v_2+v_3}{2},… ]$。 xgboost如何处理缺失值？在训练阶段，把缺失样本分别放入左节点和右边节点，通过结构分数，进行最终选择。在测试阶段，默认放入右边。 xgboost为什么能够并行？在选择要分裂的节点、选择特征和选择要分隔属性值时，都可以进行并行。 回归树的工作原理？CART用作分类时，使用基尼系数选择特征和切分点；当CART用作拟合时，使用均方差选择特征和切分点。 如何选择分裂特征和特征值？遍历每一个叶子节点中的属性和属性分裂点。 叶子结点的预测值如何选择？分裂后的树决定后，利用该公式计算：$w_j=\frac{G_j}{H_j+\lambda}, G_j=\sum_{i\in{I_j}}g_i,H_j=\sum_{i\in{I_j}}h_i$。$I_j$表示落入第j个叶子节点的样本集合，$H_j$也就是指把该叶子节点的$h_i$累加起来。 xgboost的求每棵树的复杂度公式时，为什么要上该叶子节点在的取值平方？xgboost数学推到？ 数学推到的意义在于，得到分裂后形成树的打分公式和叶子节点的预测分。通过这个打分公式来判断每次分裂的好坏，类似ID3的熵增益，C4.5的熵增益率，cart的gini系数。 首先，定义损失函数：$Obj(\theta) = \sum_{i=1}^nl(y_i,\hat{y})+\sum_{k=1}^K\Omega(f_k)$； 再定义复杂度公式：$\Omega(f_k)={\gamma}T+\frac{1}{2}\sum_{j=1}^Kw_j^2$; 再把损失函数Obj作泰勒展开，$f_k(x)当做{\Delta}x$,并提取合并一些常量，得到：$Obj(\theta) = \sum_{i=1}^n[g_if_t(x)+\frac{1}{2}h_if_t(x)^2]+{\gamma}T+\frac{1}{2}{\lambda}\sum_j^Tw_j^2$; 再用叶子节点的值来表示Obj，得到$Obj(\theta) = \sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_j^2]+{\gamma}T$ 打分公式：$Obj^{\ast}=-\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j+\lambda}+{\gamma}T$ 叶子节点预测分公式：$w_j=\frac{G_j}{H_j+\lambda}$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Deep Q-Learning]]></title>
      <url>%2F2019%2F04%2F29%2FDeep-Q-Learning%2F</url>
      <content type="text"><![CDATA[Q-learning存在的问题？ 现实生活中经常遇到模型状态太多，以至于无法构建Q(s,a)表格，也叫Q-table。 如何解决上面的问题？ 利用一个函数近似代替。例如，找到一个函数f(s,a)，是的Q(s,a) = f(s,a)。 如何构造函数的输入？ 一种方法是把状态S和动作a拼接起来用作f的输入。另一种方法是，只把s当作f的输入，f的输出是一个向量，向量的每一个维度的值代表对应动作的Q(s,a)值。 如何构建函数f？ 函数拟合是深度学习的强项，所以可以通过一个网络构建函数f，该网络叫做Q-Network(Q网络)。 Q-Learning算法原理？ 初始化矩阵Q，Q矩阵有S行A列，S为状态的数量，A为动作的数量。 while Q 未收敛： 开始一轮训练 while 状态S != 结束： 基于策略，获得动作a agent采用动作a,获得新的状态S_new,与奖励R(S,a) 更新Q矩阵，Q[S,a] = (1-k)*Q[S,a] + k*(R(S,a) + t*max(Q[S_new,:]))。Q[S,a]是矩阵第S行第a列的元素，max(Q[S_new,:])表示第S_new行最大的值。 S = S_new如何训练Q-network？ 每次和环境交互得到的奖励和新的状态都保存起来。用作训练数据。损失函数$L(w)=E[(r +{\lambda}*max_{a^{‘}}Q(s^{‘},a^{‘},w)-Q(s,a,w))^{2}]$ 如何构建Q-network的训练数据？ agent在状态S时，采取动作a后，得到新的状态S_new、奖励R(S,a)和是否结束的标志。构成一个5元组（S, a, R(s, a), S_new, is_end） 每一个5元组都可以转换一个训练样本（S, y）。当is_end为True的时候，y = R(s, a); 当is_end为False的时候，y=R(S,a)+k maxQ(S)。Q代表Q网络。 DQN的训练流程：&emsp;&emsp;算法符号介绍：迭代轮数$T$，状态特征维度$n$, 动作集$A$, 步长$\alpha$，衰减因子$\gamma$, 探索率$\epsilon$, Q为网络结构, 批量梯度下降的样本数m。&emsp;&emsp;算法输出：训练好的Q网络。&emsp;&emsp;1. 初始化Q，清空经验回放集合D。&emsp;&emsp;2. for i from 1 to T, 迭代训练：&emsp;&emsp;&emsp;&emsp;a. 初始化启始状态S&emsp;&emsp;&emsp;&emsp;b. 经过Q网络，采用$\epsilon$-贪婪策略，得到输出动作a&emsp;&emsp;&emsp;&emsp;c. 执行动作a，得到新的状态$S_{new}$、奖励$R(S,a)$和$is_end$的标志&emsp;&emsp;&emsp;&emsp;d. 将S、a、R(S,a)、$S_{new}$、$is_end$合并成5元组（S,a, R(S,a), $S_{new}$, is_end），再加入到经验回放集D。&emsp;&emsp;&emsp;&emsp;e. 从经验回放集D中，随机挑选m个样本，每个样本对应一条训练数据$(S,y)$.&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$y = \begin{cases}R(S,a) &amp; is_end = True \R(S,a) + maxQ(S) &amp; is_end = False\end{cases}$&emsp;&emsp;&emsp;&emsp;f. 利用m个样本，并基于反向传播，训练Q网络。&emsp;&emsp;&emsp;&emsp;g. 如果is_end为真，则结束本轮循环。否则，令$S=S_new$，并再从第b步开始执行。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[top命令常用操作]]></title>
      <url>%2F2019%2F04%2F25%2Ftop%E5%91%BD%E4%BB%A4%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%2F</url>
      <content type="text"><![CDATA[M: 按照内存排序。 P: 按照cpu排序 e: 改变内存显示单位。 列名=字符串a: 只看某列包含字符串a的进程。首先输入小写字母o；再输入列名=字符串，例如只想看进程名为name_a的进程，COMMAND=name_a]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hexo使用总结]]></title>
      <url>%2F2019%2F04%2F24%2Fhexo%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93%2F</url>
      <content type="text"><![CDATA[hexo博客迁移原理和参考文章：https://www.jianshu.com/p/fceaf373d797第一步：安装node.js。第二部：安装hexo。参考官网安装教程。第三步：从github中的个人主页仓库clone到本地：https://github.com/450586509/450586509.github.io.git第四步：切换到hexo源码分支。运行hexo g &amp; hexo s,检查命令是否正常工作。启动后，从浏览器中登录http://localhost:4000,查看博客是否正常显示。 更新主页文章 更新文章分为四步： 创建文章：命令：hexo new “你的文章名称”。该命令会在你hexo项目的source/_post目录下，生成”你的文章名称”.md的markdown文件。 编辑文件。利用markdown工具，编辑source/_post/“你的文章名称”.md的文件 预览文章。命令：hexo generate 和 hexo server。先利用hexo generate生成文章，再利用hexo server命令启动本地服务器。在浏览器中输入本地博客地址，可以查看文章的效果。 部署发布文章。命令：hexo g -d。当文章编写完成之后，运行该命令，就会生成并发布文章到你的个人博客网站。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kmeans++]]></title>
      <url>%2F2016%2F12%2F16%2Fkmeans%20%2B%2B%2F</url>
      <content type="text"><![CDATA[k-means++:the advantages of careful seeding kmeans++的优点：相对于kmeans，既能提升准确率也能加快速度。 By augmenting k-means with a simple, randomized seeding technique, we obtain an algorithm that is O(log k)-competitive with the optimal clustering. Experiments show our augmentation improves both the speed and the accuracy of k-means, often quite dramatically. kmeans和kmenas++介绍$c:$某一个中心点$D(x):$样本点x到已取中心点的最短距离。k-means算法很容易理解：k-means++算法： kmeans与kmeans++的区别kmeans随机选取k个中心点，kmeans以一种更加合理的策略选择。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CNN情感分析]]></title>
      <url>%2F2016%2F12%2F09%2FCNN%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[前言最近一直学习如何利用卷积神经网络（CNN）进行情感分析，一下是我用keras实现Yoon Kim 论文的过程，趟过一个大坑，在此记录一下。 算法介绍该论文是将CNN引入文本分类的开山之作，论文提出了4种模型，模型简单，效果喜人。CNN的输入时二维或以上的张量（tensor），张量不准确的表述为二维或者以上的矩阵。若利用CNN对文本数据建模，需要将输入文本转为张量的形式，利用Word Embedding 可以将单个句子转为一个矩阵,每一行对应该词的词向量。如下图： ![image_1b3eq53cv7j410j51t1fc3v1ku0m.png-35.4kB][1] #### random模型 利用随机初始化的词向量构造句子矩阵，词向量的值作为模型的参数。[keras的实现代码](https://github.com/450586509/DLNLP/blob/master/src/notebooks/CNN/cnn_static_word.ipynb) #### static 模型 利用训练好的词向量构造句子矩阵，词向量不作为模型的参数。论文中使用的是google基于新闻语料训练的词向量[keras的实现代码](https://github.com/450586509/DLNLP/blob/master/src/notebooks/CNN/cnn_static_word.ipynb)。 #### non-static模型 利用训练好的词向量构造句子，词向量作为模型的参数。[keras的实现代码](https://github.com/450586509/DLNLP/blob/master/src/notebooks/CNN/cnn_static_word.ipynb)。 #### multi channel 模型 CNN的输入有两个通道，前面三种模型的输入都是num\_words * embedding\_dim的矩阵,该模型的输入为2 * num\_words * embedding\_dim 的张量。类似于图片RGB三个通道。 第一个通道（第一个矩阵）由训练好的词向量初始化，且这些词向量作为模型参数。第二个通道（第二个矩阵）由训练好的词向量促使化，词向量保持不变，不作为模型的参数。[keras的实现代码](https://github.com/450586509/DLNLP/blob/master/src/notebooks/CNN/cnn_static_word.ipynb)（可能与原论文有出入）。 收获 在利用keras的Embedding层做static模型时，我们会构建一个word到index的字典，记为word_index_dict,再通过训练好的词向量够着要给index到词向量的字典，记为index_embedding。再构造一个矩阵weights,使得weights[i] = index_embedding[i]即可。通过Embedding层的trainable设置该层的参数是否参与训练（也就是词向量是否当作参数训练） 在构建Embedding层权重的weights时，肯定会出现某些word没有出现在google的词向量中，这时合理的初始化方法，对收敛和最后的最优值有决定性的作用。这是做这个实验所遇到最大坑，我试过[-1,1]的均匀分布、正态分布和全0，一直无法达到作者的最优值（48%）。一定要按如下初始化，使得与训练好的词向量具有相似的变化范围,作者在原码中说的是，the unknown vectors have (approximately) same variance as pre-trained ones np.random.uniform(-0.25,0.25,300)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[好好]]></title>
      <url>%2F2016%2F12%2F07%2F%E5%A5%BD%E5%A5%BD%2F</url>
      <content type="text"><![CDATA[今天听到这首五月天的新歌歌词好好，旋律好好 想把你写成一首歌想养一只猫 （主子的好，谁养谁知道）想要回到每个场景拨慢每只表（把那些好的坏的都擦掉） 我们在小孩和大人的转角盖一座城堡（城管来了）我们好好 好到疯掉像找回失散多年的双胞 生命再长不过 烟火落下了眼角世界再大不过 你我凝视的微笑在所有流失的风景与人群中 你对我最好一切好好 是否太好 没有人知道 你和我背着空空的书包逃出名为日常的监牢（背着厚厚的书包，怎么逃得掉）忘了要长大 忘了要变老忘了时间有脚 最安静的时刻 回忆 总是最喧嚣（最匆忙的时刻 回忆 总是被忘掉）最喧嚣的狂欢 寂寞 包围着孤岛还以为驯服想念能陪伴我 像一只家猫（主子岂能驯服 只好 自己举手投降）它就窝在 沙发一角 却不肯睡觉 你和我曾有满满的羽毛跳着名为青春的舞蹈不知道未来 不知道烦恼不知道那些日子 会是那么少 时间的电影 结局才知道原来大人已没有童谣最后的叮咛 最后的拥抱我们红着眼笑 我们都要把自己照顾好好到遗憾无法打扰好好的生活 好好的变老好好假装自己已经把你忘掉 :)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[渗透测试--Penetration testing]]></title>
      <url>%2F2016%2F11%2F30%2F%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95---Penetration%20testing%2F</url>
      <content type="text"><![CDATA[什么是渗透测试对获得攻击授权的系统进行安全攻击，旨在发现系统漏洞。渗透测试要获取授权，否则可能会有牢狱之灾 什么是漏洞（vulnerability）漏洞：能够对系统造成潜在攻击的地方。常见的漏洞包括：简单的密码（weak password）,缓存溢出（buffer overflows）,SQL注入。 什么是exploit？exploit: 指利用计算机系统的漏洞获取控制权的这一系列行为。 什么是payload?payload是一段代码，这段代码能控制被exploit的系统。打个比方：exploit把payload装入一个背包，当exploit攻入一个系统后，把背包放在系统。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bi-LSTM CRF 模型]]></title>
      <url>%2F2016%2F11%2F27%2FBi-LSTM%20CRF%20%E6%A8%A1%E5%9E%8B%2F</url>
      <content type="text"><![CDATA[主题最近看了一篇关于命名实体论文，后来有人把这篇论文中的Bi-LSTM + CRF模型用来做分词，并且据说取得非常好的效果，但是今天又有人说效果没有说的那么好。这个模型成功引起了我的注意，下面是阅读的一些收获吧。 Bi-LSTM+CRF模型Bi-LSTM模型在深度学习中很常用，网上有很多介绍。我倒是第一次见到它与CRF结合的情形。模型如下图 ![Bi-LSTM+CRF][1] CRF层在做什么？这里有篇非常好的CRF文章。本文模型中用的是linear-chain CRF linear-chain CRF的优点： 假设CRF进行序列标记，输入序列为$(x_1,x_2,…,x_n)$,CRF标记后的序列为$(y_1,y_2,…,y_n)$,CRF不仅能够捕获$x_i$对$y_i$的影响，也能捕获$y_{i-1}$对$y$的影响。chain-CRF假设如下，这个公式在论文中被用作序列打分函数：$P(y|x,\lambda) \propto \exp(\sum_j(\lambda_jt_j(y_{i-1}，y_i,x,i))+\sum_k(u_ks_k(y_i,x,i)))$$\tag{1}$ 论文中提到两个矩阵，分别为P和A。p的含义： P的形状是$n*k$,k是标签的数目，命名实体识别本质上是一个打标签的过程，$P_{i,j}$对应输入句子中第 $i$ 个词对第 $j$ 个标签的打分，（要点来了）P矩阵就是Bi-LSTM的输出矩阵。 A的含义： $A_{i,j}$代表标签i到标签j到转移概率，这在HMM模型中叫做状态转移矩阵。 打分函数S: 文中给出了一个打分函数，也就对于每一个预测序列$y$都有一个对应的分数。$score(X,y) = \sum^n_0A_{y_i,y_{i+1}}+\sum^n_1P_{i,y_i}$这个公式正好与(1)一样。 转为概率将打分函数的结果通过一个softmax操作，转为该序列是目标序列的概率： ![image_1b2idmh4p11siie95go8kt1ks913.png-5.5kB][2] 参数训练在训练过程中，可以通过cross-entropy损失函数，最大化正确序列的对数概率 ![image_1b2idqtau13842hp8aa134s71g1g.png-12.7kB][3] 总结一下CRF做了什么CRF层把Bi-LSTM的输出当做一个词对每个标签打分，再利用打分函数对每一个预测序列进行打分，分数越高则对应预测的概率越大，再通过一个softmax过程将分数转为概率。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[牛顿迭代法和随机梯度下降法的区别]]></title>
      <url>%2F2016%2F11%2F26%2F%E7%89%9B%E9%A1%BF%E8%BF%AD%E4%BB%A3%E6%B3%95%E5%92%8C%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
      <content type="text"><![CDATA[最本质的区别牛顿迭代法 利用迭代点 $x$ 邻域内的二阶泰勒展开式$y$去近似代替原目标函数f(x),再求$y$的极小值点$x_{new}$作为下一个迭代点 随机梯度下降法 利用迭代点 $x$ 邻域内的一阶泰勒展开式$y$去近似代替原目标函数f(x),再求$y$的极小值点$x_{new}$作为下一个迭代点 由于函数的二阶泰勒展开式比一阶更加准确所以牛顿迭代法能够更快收敛。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[矩阵求导公式]]></title>
      <url>%2F2016%2F11%2F26%2F%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E5%85%AC%E5%BC%8F%2F</url>
      <content type="text"><![CDATA[1.标量$y$对向量$\vec{X}$求导设$\vec{X}为$： ![image_1b2gfbd2r1qr13dnbr41e5uoldm.png-7.1kB][1] $\tag{1}$则，$\frac{\partial{y}} { \partial{\vec{X}}} = $ ![image_1b2gfimvl1ea91ula1aji1t2vs1313.png-14.7kB][2] 2.向量对标量求导假设$\vec{Y}$的元素都为一元函数，都以变量 $x$ 为自变量设$\vec{Y}为$： ![image_1b2gg43or1k9t1ts17rcj401pt49.png-7.3kB][3] $\tag{2}$ 则$\frac{\partial{ \vec{Y}}} { \partial{x}} = $ ![image_1b2gfr3kc1qb1gse6ppmok18gl1g.png-14.7kB][4] 3.向量对向量求导假设$\vec{Y}$的元素都为多元函数，都以变量 $\vec{X}$ 为自变量。$\vec{X}$ 和$\vec{Y}$分别为$(1),(2)$则 $\frac{\partial{ \vec{Y}}} { \partial{ \vec{X}}} = $ ![image_1b2gg0mc51bsp1asrjn91jajhud1t.png-34.5kB][5] 4.矩阵对标量求导![image_1b2ggfrvp1elpk3kv9n1re31p59m.png-25.4kB][6] 5.标量对矩阵求导标量可以理解为多元标量函数，矩阵为全部自变量。 ![image_1b2gghskoot3oqhd551m1o137o13.png-23.3kB][7]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Yan LeCun 关于深度学习的思考]]></title>
      <url>%2F2016%2F08%2F13%2FYan-LeCun-%E5%85%B3%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
      <content type="text"><![CDATA[Session with Yann LeCunWhat are some recent and potentially upcoming breakthroughs in deep learning? The most important one, in my opinion, is adversarial training (also called GAN for Generative Adversarial Networks). 对抗训练 What are your recommendations for self-studying machine learning? ###网上资料课程一大堆 What are the likely AI advancements in the next 5 to 10 years? deep learning combined with reasoning and planning.深度学习与推理和计划的结合。 深度强化学习（deep model-based reinforcement learning） 通过记忆网络增强的RNN。 基于对抗训练的生成和预测模型 可微编程。把程序视为可微的图模型，并能通过backprop进行训练。 层次计划和层次强化学习，将一个复杂的学习问题分解成简单的任务。 无监督的方法学习预测模型。###如果以上几个方面有明显的发展，那么会出现相当智能的系统### What are some recent and potentially upcoming breakthroughs in unsupervised learning?(无监督算法最近最有潜力的突破) Adversarial Training What’s your advice for undergraduate student who aspires to be a research scientist in deep learning or related field one day?(对于那些想要搞深度学习的大学生，你有什么建议？) take all the continuous math and physics class you can possibly take. If you have the choice between “iOS programming” and “quantum mechanics”, take “quantum mechanics”. In any case, take Calc I, Calc II, Calc III, Linear Algebra, Probability and Statistics, and as many physics courses as you can. But make sure you learn to program.（多学数学和物理，并且学好编程）独立思考，阅读文献验证自己的想法，找大牛带你（教授，博士），读个博士。如果博士没有申请成功，出来工作，再申请。 What is your favorite machine learning algorithm? BP]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[pycaffe]]></title>
      <url>%2F2016%2F07%2F21%2Fpycaffe%2F</url>
      <content type="text"><![CDATA[利用python加载已经训练好的caffe模型。非常简单。123456789// codeimport caffeprotoPath = &apos;G:/code/caffeDemo/lenet_train_test.prototxt&apos; weigths = &apos;G:/code/caffeDemo/_iter_10000.caffemodel&apos;net = caffe.Net(protoPath,weigths,caffe.TEST)#通过net可以查看每层的weights和所有的blobs.net.paramnet.blobs]]></content>
    </entry>

    
  
  
</search>
